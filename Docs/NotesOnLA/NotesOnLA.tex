\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\title{Notes On Linear Algebra}
\author{Spencer T. Parkin}

\newcommand{\G}{\mathbb{G}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\B}{\mathbb{B}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{identity}{Identity}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{result}{Result}[section]

\begin{document}
\maketitle

This paper is a formal compilation of all my notes on linear algebra.

Linear algebra is the study of linear functions defined on linear spaces.
Linear spaces are more often refered to as vector spaces, suggesting
to the mind a geometric interpretation of the elements of such spaces.
In fact, what we'll find is that geometric algebra facilitates the study
of linear algebra.

Holding the definitions of a vector space and linear independence as
already known, we begin with a formal definition of a linear function.
We arbitrarily define all vector spaces over the field of real numbers $\R$.
\begin{definition}
A function $f:\A\to\B$ is a mapping from a vector space $\A$ to
a vector space $\B$ with the property of preserving both scalar-vector
multiplication and vector addition.  That is, for any scalar $\lambda\in\R$,
and any two vectors $x,y\in\A$, we have $f(\lambda x)=\lambda f(x)$ and
$f(x+y)=f(x)+f(y)$.
\end{definition}
It is not enitrely clear to me how much, if any, loss in generality we incur
by restricting our study of such functions to those that map to and from
the same vector space.  Nevertheless, since this is the class of linear
functions for which I am most interested, we will proceed with this restriction.

That said, let $f:\V^n\to\V^n$ be the linear function we will study, where $\V^n$
denotes an $n$-dimensional vector space.  Then, for any $x\in\V^n$, right
away we learn two interesting things about linear functions.  Letting $\{e_i\}_{i=1}^n\subset\V^n$
be any set of $n$ linearly independent vectors taken from $\V^n$, we have
\begin{equation}\label{equ_linear_transform}
f(x) = f\left(\sum_{k=1}^n x_i e_i\right) = \sum_{k=1}^n x_i f(e_i),
\end{equation}
where $x_i = x\cdot e_i$.  The first thing this shows is that any linear function
is determined entirely by how it transforms the set of basis vectors $\{e_i\}_{i=1}^n$,
so that when we're faced with formulating a linear transformation, we need only
consider how it transforms a basis of $\V^n$.  The second thing this shows is
that every linear function is a change of basis transformation.  That is, the
set of coordinates $\{x_i\}_{i=1}^n$ for a vector $x$ are preserved while
the set of basis vectors $\{e_i\}_{i=1}^n$ are replaced with a new set
of vectors $\{f(e_i)\}_{i=1}^n$.

Equation $\eqref{equ_linear_transform}$ also brings to bair immediate
implications on the invertibility of $f$.  That is, we have enough to
prove at this point that $f$ is invertible if and only if $f$ preserves
linear independence in the sense that if $\{e_i\}_{i=1}^n$ is a
linearly independent set, then so is $\{f(e_i)\}_{i=1}^n$.  Many graphics
transformations can be easily formulated this way.

Recall that one direction of the statement $x=y\iff f(x)=f(y)$ is the
requirement of a well defined function, while the other direction is optional,
and is the condition upon which $f^{-1}$ exists.  Specifically, if there
exist distinct vectors $x,y\in\V^n$ such that $f(x)=f(y)$, then
$f^{-1}$ does not exist.  If $z=f(x)=f(y)$, then do we let $f^{-1}(z)=x$
or $f^{-1}(z)=y$?

Suppose for the moment that the set $\{f(e_i)\}_{i=1}^n$ is linearly dependent.
Then, without loss of generality, we can write $f(e_n)$
as $\sum_{i=1}^{n-1}\lambda_i f(e_i)$, where each $\lambda_i\in\R$.
Now let $x_n=0$ and let $y\in\V^n$ be $\sum_{i=1}^n y_i e_i$, where $y_n\neq 0$,
and for all $i<n$, let $y_i=x_i-\lambda_i y_n$.  Clearly $x\neq y$, and we see that
\begin{align*}
f(x) &= \sum_{i=1}^{n-1} x_if(e_i)
 = \sum_{i=1}^{n-1}(y_i+\lambda_i y_n)f(e_i) \\
 &= \sum_{i=1}^{n-1}y_i f(e_i) + y_n\sum_{i=1}^{n-1}\lambda_i f(e_i)
 = \sum_{i=1}^{n-1}y_i f(e_i) + y_n f(e_n)
 = f(y),
\end{align*}
showing that $f$ is non-invertible.

Now suppose that $\{f(e_i)\}_{i=1}^n$ is linearly independent.  We must show
that for any $x,y\in\V^n$, if $f(x)=f(y)$, then $x=y$.  This follows immediately
from the equation
\begin{equation*}
0 = f(x)-f(y) = \sum_{i=1}^n(x_i-y_i)f(e_i),
\end{equation*}
because we must have for all integers $i\in[1,n]$, $x_i=y_i$ on the grounds
that $\{f(e_i)\}_{i=1}^n$ is a linearly independent set.

Having now established the conditions upon which $f^{-1}$ exists, let's quickly
prove the uniqueness of $f^{-1}$.  Suppose the functions $g$ and $h$ are
distinct inverses of $f$.  By distinct, this must mean that there exists $y\in f(\V^n)$
such that $g(y)\neq h(y)$.  Let $x\in\V^n$ be such that $f(x)=y$.  We then
have $x=g(f(x))=g(y)\neq h(y)=h(f(x))=x$, which is a contradiction.
Inverses of functions in general are therefore unique.

We have now satisfied the basic questions of existance and uniqueness
for inverses of linear functions.  Given a linear function $f$, what we
would now hope to be able to do is find $f^{-1}$.
This is where geometric algebra comes in.
\begin{definition}
A linear function $f$ is also called an outermorphism if it preserves
the outer product.  That is, for any two vectors $x,y\in\V^n$,
we have $f(x\wedge y)=f(x)\wedge f(y)$.
\end{definition}


\end{document}